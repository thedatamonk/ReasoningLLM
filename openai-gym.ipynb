{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bede05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dbee896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now make the env\n",
    "# env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# print (f\"All possible actions: {env.action_space}\")\n",
    "# print (f\"All possible states: {env.observation_space}\")\n",
    "\n",
    "# observation, info = env.reset(seed=42)\n",
    "\n",
    "# print (f'STATE[INITIAL]: {observation}')\n",
    "# for _ in range(1000):\n",
    "#     # insert your learned policy\n",
    "#     action = env.action_space.sample()\n",
    "#     print (f\"ACTION: {action}\")\n",
    "\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "#     print (f'STATE[CURR]: {observation} - TERMINATED: {terminated} - TRUNCATED: {truncated}')\n",
    "\n",
    "#     # if episode has ended or truncated\n",
    "#     # TODO: Diff b/w terminated and truncated\n",
    "#     if terminated or truncated:\n",
    "#         observation, info = env.reset()\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ea9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "config  = {\n",
    "    \"gamma\": 0.99, # discount factor used for discounting future rewards\n",
    "    \"clip_epsilon\": 0.2,   # used in the clipping function\n",
    "    \"rollout_length\": 2048,  # number of timesteps we want the Actor to play with the environment for\n",
    "    \"epochs\": 100,    # number of epochs used for PPO update step\n",
    "    \"batch_size\": 64, # batch size to use in PPO update\n",
    "    \"lr\": 1e-4, # learning rate for PPO update \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1111a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 4\n",
      "Action dimension: 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print (f\"State dimension: {state_dim}\")\n",
    "print (f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758305fd",
   "metadata": {},
   "source": [
    "## Policy network\n",
    "\n",
    "- Also known as actor network.\n",
    "- The goal is to learn a policy that the \"Actor\" will use to interact with the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15aef8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "    def forward(self, state):\n",
    "        # Takes as input a state and returns the action logits\n",
    "        return self.net(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d21a0d",
   "metadata": {},
   "source": [
    "## Value network\n",
    "\n",
    "- Also known as critic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5cdf1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        # this function takes as input a state and returns the value of that state\n",
    "        return self.net(state).squeeze(-1) # remove the last dimension of size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629bcb0e",
   "metadata": {},
   "source": [
    "# Initialise the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c1d4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "value_net = ValueNetwork(state_dim)\n",
    "policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=config['lr'])\n",
    "value_optimizer = torch.optim.Adam(value_net.parameters(), lr=config['lr'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16cdb1a",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191b10a",
   "metadata": {},
   "source": [
    "### Advantage function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ad29fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards to go: tensor([ 1.1179,  1.3102, -2.1089, -1.2098,  3.2250,  2.5000])\n",
      "Advantages: tensor([ -4.8821,  -4.1898,   1.3911, -10.2098,  -5.2750,  -7.5000])\n"
     ]
    }
   ],
   "source": [
    "def calc_advantages(rewards, values, gamma=0.09):\n",
    "    # first calculate the rewards-to-go at each step\n",
    "    # we already are given the expected rewards AKA values at each step\n",
    "    # advantage of each step is the differennce b/w the rewards-to-go and the value of that step\n",
    "    cuml_reward= 0\n",
    "    rewards_to_go = []\n",
    "    for reward in reversed(rewards):\n",
    "        cuml_reward = reward + gamma * cuml_reward\n",
    "        rewards_to_go.insert(0, cuml_reward)\n",
    "    \n",
    "    # convert rewards_to_go and values to tensors\n",
    "    rewards_to_go = torch.tensor(rewards_to_go, dtype=torch.float32)\n",
    "    values = torch.tensor(values, dtype=torch.float32)\n",
    "\n",
    "    # calculate advantages for each time step\n",
    "    advantages = rewards_to_go - values\n",
    "    return advantages, rewards_to_go\n",
    "\n",
    "\n",
    "def test_calc_advantages():\n",
    "    rewards = [1.0, 1.5, -2.0, -1.5, 3.0, 2.5]\n",
    "    values = [6.0, 5.5, -3.5, 9.0, 8.5, 10.0]\n",
    "    advantages, rewards_to_go = calc_advantages(rewards, values)\n",
    "\n",
    "    print(\"Rewards to go:\", rewards_to_go)\n",
    "    print(\"Advantages:\", advantages)\n",
    "\n",
    "test_calc_advantages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d64360",
   "metadata": {},
   "source": [
    "### PPO update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3205e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/20 - Policy Loss: 4.604244232177734 - Value Loss: 448.4206237792969\n",
      "Iteration 2/20 - Policy Loss: 4.408319473266602 - Value Loss: 367.6126708984375\n",
      "Iteration 3/20 - Policy Loss: 3.796520471572876 - Value Loss: 2441.7431640625\n",
      "Iteration 4/20 - Policy Loss: 3.0511834621429443 - Value Loss: 4432.21533203125\n",
      "Iteration 5/20 - Policy Loss: 2.7016100883483887 - Value Loss: 4440.46142578125\n",
      "Iteration 6/20 - Policy Loss: 3.447909355163574 - Value Loss: 4865.9033203125\n",
      "Iteration 7/20 - Policy Loss: 4.083921432495117 - Value Loss: 4731.8359375\n",
      "Iteration 8/20 - Policy Loss: 4.270987510681152 - Value Loss: 4815.7451171875\n",
      "Iteration 9/20 - Policy Loss: 4.466119766235352 - Value Loss: 4901.462890625\n",
      "Iteration 10/20 - Policy Loss: 4.504435062408447 - Value Loss: 4815.451171875\n",
      "Iteration 11/20 - Policy Loss: 4.4711689949035645 - Value Loss: 4833.5751953125\n",
      "Iteration 12/20 - Policy Loss: 4.628590106964111 - Value Loss: 4879.32958984375\n",
      "Iteration 13/20 - Policy Loss: 4.575385093688965 - Value Loss: 4914.0771484375\n",
      "Iteration 14/20 - Policy Loss: 4.535947799682617 - Value Loss: 4882.40234375\n",
      "Iteration 15/20 - Policy Loss: 4.51458740234375 - Value Loss: 4881.646484375\n",
      "Iteration 16/20 - Policy Loss: 4.558011054992676 - Value Loss: 4893.27294921875\n",
      "Iteration 17/20 - Policy Loss: 4.580988883972168 - Value Loss: 4869.19970703125\n",
      "Iteration 18/20 - Policy Loss: 4.516817092895508 - Value Loss: 4899.36962890625\n",
      "Iteration 19/20 - Policy Loss: 4.512733459472656 - Value Loss: 4826.466796875\n",
      "Iteration 20/20 - Policy Loss: 4.53564977645874 - Value Loss: 4877.94775390625\n"
     ]
    }
   ],
   "source": [
    "NUM_ITERATIONS = 20\n",
    "\n",
    "for iter_idx in range(NUM_ITERATIONS):\n",
    "\n",
    "    states, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "    state, _ = env.reset()   # initial state\n",
    "\n",
    "    for _ in range(config['rollout_length']):\n",
    "        # get the action from the current policy\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        action_logits = policy_net(state_tensor)\n",
    "        action_dist = torch.distributions.Categorical(logits=action_logits) # get the action distribution\n",
    "        action = action_dist.sample()   # use the distribution to sample an action\n",
    "\n",
    "\n",
    "        # pass the action to the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "        states.append(state_tensor)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(action_dist.log_prob(action))\n",
    "        values.append(value_net(state_tensor).item())\n",
    "\n",
    "        state = next_state if not terminated and not truncated else env.reset()[0]\n",
    "    \n",
    "    # convert the collected data to tensors\n",
    "    states = torch.stack(states)\n",
    "    actions = torch.stack(actions)\n",
    "    old_log_probs = torch.stack(log_probs).detach()\n",
    "    # we will use the collected rewards and values to calculate the advantages\n",
    "    advantages, rewards_to_go = calc_advantages(rewards, values, config['gamma'])\n",
    "\n",
    "    # TODO: Why is this necessary?\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8) # normalize the advantages\n",
    "\n",
    "    # now we have to update the policy and value networks\n",
    "    # using rollout data we collected\n",
    "    # we will show the rollout data to the policy and value networks multiple times (epochs) times\n",
    "    # so the networks can learn different aspects from the collected rollout data\n",
    "    for _ in range(config['epochs']):\n",
    "        # We will do batch updates to train the forward and backward passes faster\n",
    "        for i in range(0, config['rollout_length'], config['batch_size']):\n",
    "            idx = slice(i, i + config['batch_size'])\n",
    "            batch_states = states[idx]\n",
    "            batch_actions = actions[idx]\n",
    "            batch_advantages = advantages[idx]\n",
    "            batch_old_log_probs = old_log_probs[idx]\n",
    "            batch_rewards_to_go = rewards_to_go[idx]\n",
    "\n",
    "            # get the action logits for the batch from the updated policy\n",
    "            new_action_logits = policy_net(batch_states)\n",
    "            new_action_dist = torch.distributions.Categorical(logits=new_action_logits)\n",
    "            new_log_probs = new_action_dist.log_prob(batch_actions)   # get the log probabilities of the actions using the new policy\n",
    "\n",
    "            ratio = (new_log_probs - batch_old_log_probs).exp()   # this can also be written as ratio = new_probs/old_probs\n",
    "            unclipped = ratio * batch_advantages\n",
    "            clipped = torch.clamp(ratio, 1 - config['clip_epsilon'], 1 + config['clip_epsilon']) * batch_advantages\n",
    "            policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "\n",
    "            value_preds = value_net(batch_states)\n",
    "            value_loss = nn.MSELoss()(value_preds, batch_rewards_to_go)\n",
    "\n",
    "            policy_optimizer.zero_grad() # just zero out the gradients before the backward pass\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            value_optimizer.zero_grad() # just zero out the gradients before the backward pass\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "\n",
    "        \n",
    "    print (f\"Iteration {iter_idx+1}/{NUM_ITERATIONS} - Policy Loss: {policy_loss.item()} - Value Loss: {value_loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66041307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial observations are - \n",
    "# 1. The value loss is very high > 4000\n",
    "# 2. We also need to continuously visualise the losses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
