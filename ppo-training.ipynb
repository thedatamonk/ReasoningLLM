{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pretrained llm or even a SFT llm from huggingface - let's call it the BaseLLM\n",
    "# Have a dataset and evaluate the BaseLLM on the dataset - define the metrics/benchmarks that you want to use\n",
    "# RLHF setup\n",
    "    # Setup #1 - Rule based reward model with PPO\n",
    "    # Setup #2 - Model based reward model with PPO\n",
    "       # We have to collect a dataset for training the reward model - this dataset is also known as the preference dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34379083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d52837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO is on-policy: learns from current policy or current experience\n",
    "# in the contrary, DQN is off-policy: learns from past experiences as well\n",
    "\n",
    "class RobotEnv():\n",
    "    def __init__(self, goal_position=10, max_steps=100):\n",
    "        self.position = 0 # initial position of the robot\n",
    "        self.goal_position = goal_position # goal position of the robot\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0      # indicates number of steps taken by the robot\n",
    "\n",
    "    def reset(self):\n",
    "        # reset environment and actor's current state\n",
    "        self.position = 0\n",
    "        self.current_step = 0\n",
    "        return self.position\n",
    "\n",
    "    def step(self, action):\n",
    "        # compute next state based on actor's current action\n",
    "        self.position += action      # TODO: What values can action take?\n",
    "        self.current_step += 1\n",
    "\n",
    "        # compute reward based on actor's current action\n",
    "        reward = -abs(self.position - self.goal_position)    # close the robot to the goal position, higher is the reward\n",
    "        \n",
    "        # check if actor has reached final state\n",
    "        # final state is achieved if the robot has completed max_steps\n",
    "        # or it has reached the vicinity of the goal position\n",
    "        done = self.current_step >= self.max_steps or (abs(self.position - self.goal_position) < 1)\n",
    "\n",
    "        # return new state state, reward, done\n",
    "        return self.position, reward, done\n",
    "\n",
    "    def get_experiences_from_curr_policy(self, actor, device, num_episodes, max_steps):\n",
    "        \"\"\"\n",
    "        Collects experiences from the environment using the current policy.\n",
    "\n",
    "        Args:\n",
    "            actor: The actor network.\n",
    "            device: The device to run the computations on (e.g., 'cpu', 'cuda', 'mps').\n",
    "            num_episodes: The number of episodes to collect data from.\n",
    "            max_steps: The maximum number of steps per episode.\n",
    "\n",
    "        Returns:\n",
    "            A list of episodes where each episodes consists of states, actions, rewards, next states, dones and log_probs.\n",
    "        \"\"\"\n",
    "        episodes = []\n",
    "        for _ in range(num_episodes):\n",
    "            episode_data = {\n",
    "                \"states\": [],\n",
    "                \"actions\": [],\n",
    "                \"rewards\": [],\n",
    "                \"next_states\": [],\n",
    "                \"dones\": [],\n",
    "                \"log_probs\": [],\n",
    "            }\n",
    "            state = self.reset()       # start from the beginning\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done and step < max_steps:\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(device)       # convert state to tensor & add the batch dimension\n",
    "                mean, stddev = actor(state)                                    # get the mean and stddev of the action distribution given the current policy\n",
    "                distribution = torch.distributions.Normal(mean, stddev)                     # create a normal distribution\n",
    "                action = distribution.sample()                                 # sample an action from the distribution\n",
    "                log_prob = distribution.log_prob(action).sum(dim=1)                           # compute the log probability of the action. # TODO: check if sum is needed\n",
    "\n",
    "\n",
    "                # perform this action and get the reward from the environment\n",
    "                next_state, reward, done = self.step(action.cpu().detach().numpy()[0])\n",
    "\n",
    "                episode_data[\"states\"].append(state)\n",
    "                episode_data[\"actions\"].append(action.cpu().detach().numpy()[0])\n",
    "                episode_data[\"rewards\"].append(reward)\n",
    "                episode_data[\"next_states\"].append(next_state)\n",
    "                episode_data[\"dones\"].append(done)\n",
    "                episode_data[\"log_probs\"].append(log_prob.cpu().detach().numpy()[0])\n",
    "\n",
    "                # update state\n",
    "                state = next_state\n",
    "                step += 1\n",
    "\n",
    "            episodes.append(episode_data)\n",
    "\n",
    "        return episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dcf6c9",
   "metadata": {},
   "source": [
    "# Actor & Critic Networks\n",
    "\n",
    "*1. Both of these can be any neural network as long as they are fulfilling the main purpose of the Actor and Critic.*<br>\n",
    "*2. Actor uses the current policy to generate the next action.*<br>\n",
    "*3. Critic estimates the expected reward of the current state*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.mean_layer = nn.Linear(128, action_dim)\n",
    "        self.std_layer = nn.Linear(128, action_dim)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        # outputs the distribution of actions to sample from based on the current policy\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        mean = self.mean_layer(x)\n",
    "        std = F.softplus(self.std_layer(x))\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        # critic is the value function so it only needs the state_dim\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.value_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # computes the value (expected reward) of the current state of the actor\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.value_layer(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Temporal Difference Error:\n",
    "Difference b/w Actual Reward and Expected Reward of the current state\n",
    "td_error = r_t + gamma * V(t+1) - V(t)\n",
    "\n",
    "Monte Carlo Return:\n",
    "Difference b/w Actual Reward (collected for all the steps so far) and the Expected Reward of the current state\n",
    "mc_return = r_t + gamma * r_t+1 * gammae^2 * r_t+2 + ... + gamma^(T-t) * r_T - V(t)\n",
    "\n",
    "Generalized Advantage Return\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c924ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_advantage(rewards, values, dones, gamma=0.99, tau=0.95):\n",
    "    \"\"\"\n",
    "    Generalized Advantage Estimation (GAE)\n",
    "    Calculates how much better an action is wrt the average action in a given state\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i+1] + (1 - dones[i]) - values[i]\n",
    "        gae = delta + gamma * tau * (1 - dones[i]) * gae\n",
    "\n",
    "        advantages.insert(0, gae)\n",
    "\n",
    "    return advantages\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274de883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update_loop():\n",
    "    \n",
    "    num_episodes = 50\n",
    "    max_steps = 10\n",
    "\n",
    "\n",
    "    # setup device -> gpu or mps or cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    # initalise environment\n",
    "    env = RobotEnv(goal_position=10, max_steps=100)\n",
    "\n",
    "    # initialise actor network\n",
    "    actor = Actor(state_dim=1, action_dim=1).to(device) # state_dim = 1 since the robot's position is a single value\n",
    "\n",
    "    # initialise critic network\n",
    "    critic = Critic(state_dim=1).to(device)     # state_dim = 1 since the robot's position is a single value\n",
    "\n",
    "    # collect experiences\n",
    "    episode_experiences = env.get_experiences_from_curr_policy(env, actor, device, num_episodes, max_steps)\n",
    "\n",
    "    for episode_data in episode_experiences:\n",
    "        # Process each episode's data\n",
    "        advantages = calc_advantage(episode_data[\"rewards\"], ..., episode_data[\"dones\"])\n",
    "        # ... (rest of PPO update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74755d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def main():\n",
    "    # setup device -> gpu or mps or cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    # initalise environment\n",
    "    env = RobotEnv(goal_position=10, max_steps=100)\n",
    "\n",
    "    # initialise actor network\n",
    "    actor = Actor(state_dim=1, action_dim=1).to(device) # state_dim = 1 since the robot's position is a single value\n",
    "\n",
    "    # initialise critic network\n",
    "    critic = Critic(state_dim=1).to(device)     # state_dim = 1 since the robot's position is a single value\n",
    "\n",
    "    optimizer_actor = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "    optimizer_critic = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "\n",
    "    NUM_EPISODES = 1000\n",
    "    max_steps = 100\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        experiences = env.get_experiences_from_curr_policy(actor, device, )    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
