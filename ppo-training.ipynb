{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pretrained llm or even a SFT llm from huggingface - let's call it the BaseLLM\n",
    "# Have a dataset and evaluate the BaseLLM on the dataset - define the metrics/benchmarks that you want to use\n",
    "# RLHF setup\n",
    "    # Setup #1 - Rule based reward model with PPO\n",
    "    # Setup #2 - Model based reward model with PPO\n",
    "       # We have to collect a dataset for training the reward model - this dataset is also known as the preference dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34379083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d52837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO is on-policy: learns from current policy or current experience\n",
    "# in the contrary, DQN is off-policy: learns from past experiences as well\n",
    "\n",
    "class CodingSandbox():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        # reset environment and actor's current state\n",
    "        pass\n",
    "\n",
    "    def step(self, action):\n",
    "        # compute next state based on actor's current action\n",
    "\n",
    "        # compute reward based on actor's current action\n",
    "\n",
    "        # check if actor has reached final state\n",
    "\n",
    "        # return\n",
    "        pass\n",
    "\n",
    "    def get_experiences_from_curr_policy(self, actor, num_episodes, max_steps):\n",
    "        \"\"\"\n",
    "        Collects experiences from the environment using the current policy.\n",
    "\n",
    "        Args:\n",
    "            actor: The actor network.\n",
    "            num_episodes: The number of episodes to collect data from.\n",
    "            max_steps: The maximum number of steps per episode.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing lists of states, actions, rewards, next states, and dones.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodingActor(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, **params):\n",
    "        # outputs the distribution of actions to sample from based on the current policy\n",
    "        pass\n",
    "\n",
    "class CodingCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, **params):\n",
    "        # computes the value (expected reward) of the current state of the actor\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Temporal Difference Error:\n",
    "Difference b/w Actual Reward and Expected Reward of the current state\n",
    "td_error = r_t + gamma * V(t+1) - V(t)\n",
    "\n",
    "Monte Carlo Return:\n",
    "Difference b/w Actual Reward (collected for all the steps so far) and the Expected Reward of the current state\n",
    "mc_return = r_t + gamma * r_t+1 * gammae^2 * r_t+2 + ... + gamma^(T-t) * r_T - V(t)\n",
    "\n",
    "Generalized Advantage Return\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c924ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_advantage(rewards, values, dones, gamma=0.99, tau=0.95):\n",
    "    \"\"\"\n",
    "    Generalized Advantage Estimation (GAE)\n",
    "    Calculates how much better an action is wrt the average action in a given state\n",
    "    \"\"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274de883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update_loop():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74755d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
